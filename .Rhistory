getwd()
####################################################################################
# Purpose: perform dynamic prediction by landmarking with NCC and full cohort data.
# Author: Yen Chang
# Creation Date: 3/22/25
# Author: Yen Chang
####################################################################################
source('LMSub_functions.R')
####################################################################################
# Purpose: perform dynamic prediction by landmarking with NCC and full cohort data.
# Author: Yen Chang
# Creation Date: 3/22/25
# Author: Yen Chang
####################################################################################
source('LMSub_functions.R')
library(data.table)
library(survival)
library(pROC)
# Read data
fuldat = fread("fuldat.csv")
nccdat = fread("nccdat.csv")
valdat = fread("valdat.csv")
# Landmark times and horizon
landmark.time = seq(0, 10, by=2)
n.landmark.time = length(landmark.time)
horizon = c(3, 5)
# Number of controls per case
mctrl = 1
# true covariate effects
beta = matrix(1.2, n.landmark.time, 1)
beta_cor = matrix(0, n.landmark.time, 2)
beta_cor[, 1] = rep(1.3, n.landmark.time)
beta_cor[, 2] = rep(0.55, n.landmark.time)
# parameters for the (marginal) baseline hazard
lambda = 0.019; kappa = 1.5  # ~ 10% incidence at 20 years
# distribution of theta (adapted from R code accompanying Li et al. (2023))
b_beta_theta = 2.5+0.05*landmark.time
a_beta_theta = 2-0.02*landmark.time
upper.theta <- 1       # upper bound of theta;
delta.theta <- 0.002   # grid to discretize theta;
theta <- seq(delta.theta/2, upper.theta, by=delta.theta)
theta.pdf.input = matrix(min(theta), n.landmark.time, length(theta))
for(j in 1:n.landmark.time){
theta.pdf.input[j,] = beta.distribution(theta, j)
}
# Define residual times
fuldat[ , T.resi := T.obs - tj]
nccdat[ , T.resi := T.obs - tj]
valdat[ , T.resi := T.obs - tj]
# Compute inverse probability weights
fuldat0 = fuldat[tj==0, .(id, T.obs, status, Z3)]  ## Only keep info needed for computing inclusion probability
## (marginal survival times and the matching variable Z3)
fuldat0[ , matchedset := lapply(id, function(i){
if (status[i]==1) {which(T.obs>=T.obs[i] & id!=id[i] & Z3==Z3[i])} else {NA}})]
fuldat0[ , matchedset.size := ifelse(status==1, lengths(matchedset), NA)]
fuldat0[ , nrisk   := rank(-T.obs, ties.method="max")]        # number at risk at each event time
fuldat0[ , nriskZ3 := rank(-T.obs, ties.method="max"), by=Z3] # number at risk at each event time and with the same Z3
fuldat0[ , term := ifelse(status==1, 1 - as.integer(matchedset.size>=mctrl)*mctrl/matchedset.size, 1)]
setorder(fuldat0, T.obs)
## Weights for IPW-SRS
for (jj in seq_along(landmark.time)) {
fuldat0[T.obs>=landmark.time[jj], paste0("wgt", jj) := unlist(lapply(.I, function(i){
if (status[i]==1) {return(1)
} else {
idx = (Z3[i]==Z3[1:i])
return(1/(1-prod(term[1:i][idx])))}}))]
}
## Weights for IPW-IND
fuldat0[ , wgt.ind := wgt1]
# Merge IPW weights back with full cohort data
fuldat0 = melt(fuldat0, id.vars=c("id", "nrisk", "nriskZ3", "wgt.ind"), value.name="wgt.srs",
measure.vars=paste0("wgt", 1:n.landmark.time), na.rm=TRUE)
fuldat0[ , j := as.integer(substr(variable, 4, 5))]
fuldat0[ , variable := NULL]
fuldat = fuldat0[fuldat, on=.(id, j)]
# Find the maximum case time that a control is sampled (for IPW-SRS)
nccdat[ , T.case := min(T.obs), by=.(matched.id)]
maxTcase = nccdat[ , .(max.T.case = max(T.case)), keyby=.(id)]
fuldat[maxTcase, max.T.case := max.T.case, on=.(id)]
# Create an indicator for inclusion in the NCC sample (for IPW analyses)
nccid = unique(nccdat[ , id])
fuldat[ , ind.ncc := (id %in% nccid)]
# Get numbers at risk for estimating cumulative baseline hazards (for conditional likelihood)
nccdat[fuldat[status==1, ], `:=` (nrisk=nrisk, nriskZ3=nriskZ3), on=.(T.case=T.obs, j)]
# Create matrices for storing estimates
betaful = betasrs = betaind = betacon = matrix(NA, n.landmark.time, 4)
betaseful = betasesrs = betaseind = betasecon = betaful
bHazful = bHazseful = aucful = bsful = rmseful = cvrgful = matrix(NA, n.landmark.time, length(horizon))
bHazsrs = bHazsesrs = aucsrs = bssrs = rmsesrs = cvrgsrs = bHazful
bHazind = bHazseind = aucind = bsind = rmseind = cvrgind = bHazful
bHaz0con = bHaz0secon = bHaz1con = bHaz1secon = auccon = bscon = rmsecon = cvrgcon = bHazful
survful = survsrs = survind = survcon = array(NA, dim=c(nrow(valdat[tj==0, ]), n.landmark.time, length(horizon)))
survseful = survsesrs = survseind = survsecon = survful
# Iterate over all landmark times
for(jj in 1:n.landmark.time){
# Create indicators for inclusion into the jj-th landmark analysis
ind.ful = fuldat[ , tj==landmark.time[jj]]
ind.srs = fuldat[ , tj==landmark.time[jj] & ind.ncc==TRUE & is.finite(wgt.srs) & max.T.case >= landmark.time[jj]]
ind.ind = fuldat[ , tj==landmark.time[jj] & ind.ncc==TRUE & is.finite(wgt.ind)]
ind.con = nccdat[ , tj==landmark.time[jj]]
nlm = sum(ind.ful)      # size of the full cohort landmark sample
nlmsrs = sum(ind.srs)   # size of the NCC landmark SRS sample (landmarking sampled risk sets)
nlmind = sum(ind.ind)   # size of the NCC landmark IND sample (landmarking individuals)
# Maximize the likelihoods
fitful = coxph(Surv(T.resi, status) ~ Z0 + Z1 + Z2 + Z3, data=fuldat[ind.ful, ], ties="breslow")
fitsrs = coxph(Surv(T.resi, status) ~ Z0 + Z1 + Z2 + Z3, data=fuldat[ind.srs, ], ties="breslow", weights=wgt.srs)
fitind = coxph(Surv(T.resi, status) ~ Z0 + Z1 + Z2 + Z3, data=fuldat[ind.ind, ], ties="breslow", weights=wgt.ind)
fitcon = coxph(Surv(T.resi, case)   ~ Z0 + Z1 + Z2 + strata(matched.id), data=nccdat[ind.con, ], ties="breslow")
# Beta
betaful[jj, ] = beta.ful = fitful$coef
betasrs[jj, ] = beta.srs = fitsrs$coef
betaind[jj, ] = beta.ind = fitind$coef
betacon[jj, 1:3] = beta.con = fitcon$coef
# SE of beta
## IF-based SE of beta following Shin et al. (2020) for full cohort and IPW analyses
dfbeta.ful = residuals(fitful, type="dfbeta") ### IF(beta)
dfbeta.srs = residuals(fitsrs, type="dfbeta")
dfbeta.ind = residuals(fitind, type="dfbeta")
betaseful[jj, ] = sqdg(crossprod(dfbeta.ful)*nlm/(nlm-1))
noncase.srs = fuldat[ind.srs, status==0]
jcov.srs = jointVP(fuldat[ind.srs, ], m=mctrl, wgtcol="wgt.srs") ## joint inclusion probability in the landmark SRS sample
omega = t(dfbeta.srs[noncase.srs, ]) %*% jcov.srs %*% dfbeta.srs[noncase.srs, ]
betasesrs[jj, ] = sqdg(crossprod(dfbeta.srs/sqrt(fuldat[ind.srs, wgt.srs]))*nlm/(nlm-1) + omega)
noncase.ind = fuldat[ind.ind, status==0]
jcov.ind = jointVP(fuldat[ind.ind, ], m=mctrl, wgtcol="wgt.ind") ## joint inclusion probability in the landmark IND sample
omega = t(dfbeta.ind[noncase.ind, ]) %*% jcov.ind %*% dfbeta.ind[noncase.ind, ]
betaseind[jj, ] = sqdg(crossprod(dfbeta.ind/sqrt(fuldat[ind.ind, wgt.ind]))*nlm/(nlm-1) + omega)
## SE based on observed information for the conditional likelihood
betasecon[jj, 1:3] = sqrt(diag(fitcon$var))
# cumulative baseline hazards and SEs
## Increments in baseline hazards and their influence functions
failtime.ful = sort(fitful$y[fitful$y[ , 2]==1, 1])
failtime.srs = sort(fitsrs$y[fitsrs$y[ , 2]==1, 1])
failtime.ind = sort(fitind$y[fitind$y[ , 2]==1, 1])
order.ful = as.integer(names(failtime.ful))
order.srs = as.integer(names(failtime.srs))
order.ind = as.integer(names(failtime.ind))
bhaz.ful = dLambda0(as.matrix(fuldat[ind.ful, .(Z0, Z1, Z2, Z3)]), # Z matrix
dfbeta.ful,    # IF(beta)
beta.ful,      # beta
failtime.ful,  # ordered failure times
fuldat[ind.ful, T.resi], # all event times
rep(1, nlm),   # IPW weights (1 for full cohort analyses)
rep(1, fitful$nevent), # weights for cases (i.e., all 1 in our setup)
order.ful,     # vector of row indices of the ordered failure times
nlm,           # number of subjects in the landmark sample
fitful$nevent) # number of failures in the landmark sample
## This c++ function returns the estimated increments in cumulative baselines (dL0t) and
## the associated influence functions (df_dL0t)
bhaz.srs = dLambda0(as.matrix(fuldat[ind.srs, .(Z0, Z1, Z2, Z3)]),
dfbeta.srs, beta.srs, failtime.srs, fuldat[ind.srs, T.resi],
fitsrs$weights, fitsrs$weights[fitsrs$y[, 2]==1],
order.srs, nlmsrs, fitsrs$nevent)
bhaz.ind = dLambda0(as.matrix(fuldat[ind.ind, .(Z0, Z1, Z2, Z3)]),
dfbeta.ind, beta.ind, failtime.ind, fuldat[ind.ind, T.resi],
fitind$weights, fitind$weights[fitind$y[, 2]==1],
order.ind, nlmind, fitind$nevent)
bhaz.con = nccdat[ind.con, .(id, case, T.resi, Z0, Z1, Z2, Z3, nrisk, nriskZ3, matched.id)]
bhaz.con[ , `:=` (explp = exp(Z0*beta.con["Z0"] + Z1*beta.con["Z1"] + Z2*beta.con["Z2"]), wgt.lbZ3 = nriskZ3/(mctrl+1))]
bhaz.con[ , `:=` (failtime = min(T.resi), bhazZ3 = sum(case)/sum(explp*wgt.lbZ3), size = .N), by=matched.id]
for(h in seq_along(horizon)){
## Estimate the cumulative baseline hazard
bHazful[jj, h] = sum(bhaz.ful$dL0t[failtime.ful<=horizon[h]])
bHazsrs[jj, h] = sum(bhaz.srs$dL0t[failtime.srs<=horizon[h]])
bHazind[jj, h] = sum(bhaz.ind$dL0t[failtime.ind<=horizon[h]])
## stratify by the matching variable Z3 for the conditional likelihood analysis
bHaz0con[jj, h] = sum(bhaz.con[failtime<=horizon[h] & case==1 & Z3==0, bhazZ3])
bHaz1con[jj, h] = sum(bhaz.con[failtime<=horizon[h] & case==1 & Z3==1, bhazZ3])
## IF-based SE for full cohort and IPW analyses
dfbHaz.ful = rowSums(bhaz.ful$df_dL0t[ , failtime.ful<=horizon[h]])
dfbHaz.srs = rowSums(bhaz.srs$df_dL0t[ , failtime.srs<=horizon[h]])
dfbHaz.ind = rowSums(bhaz.ind$df_dL0t[ , failtime.ind<=horizon[h]])
bHazseful[jj, h] = sqrt(crossprod(dfbHaz.ful)*nlm/(nlm-1))
omega = t(dfbHaz.srs[noncase.srs]) %*% jcov.srs %*% dfbHaz.srs[noncase.srs]
bHazsesrs[jj, h] = sqrt(crossprod(dfbHaz.srs/sqrt(fuldat[ind.srs, wgt.srs]))*nlm/(nlm-1) + omega)
omega = t(dfbHaz.ind[noncase.ind]) %*% jcov.ind %*% dfbHaz.ind[noncase.ind]
bHazseind[jj, h] = sqrt(crossprod(dfbHaz.ind/sqrt(fuldat[ind.ind, wgt.ind]))*nlm/(nlm-1) + omega)
## SE for the Langholz and Borgan (1997) estimator
bhaz.con[ , termZ3 := bhazZ3^2*explp*wgt.lbZ3]
bhazZ32sum = bhaz.con[size>1, .(sum(termZ3*Z0), sum(termZ3*Z1), sum(termZ3*Z2)), by=.(Z3, failtime)]
B00 = - colSums(bhazZ32sum[failtime<=horizon[h] & Z3==0, -c("Z3", "failtime")])
B01 = - colSums(bhazZ32sum[failtime<=horizon[h] & Z3==1, -c("Z3", "failtime")])
bHaz0secon[jj, h] = sqrt(bhaz.con[failtime<=horizon[h] & case==1 & Z3==0, sum((bhazZ3)^2)] + diag(B00 %*% fitcon$var %*% B00))
bHaz1secon[jj, h] = sqrt(bhaz.con[failtime<=horizon[h] & case==1 & Z3==1, sum((bhazZ3)^2)] + diag(B01 %*% fitcon$var %*% B01))
# predict residual survival for subjects in the validation dataset
valcov = as.matrix(valdat[tj==landmark.time[jj], .(Z0, Z1, Z2, Z3)])
surv.ful = c(exp(-bHazful[jj, h]*exp(valcov %*% beta.ful)))
surv.srs = c(exp(-bHazsrs[jj, h]*exp(valcov %*% beta.srs)))
surv.ind = c(exp(-bHazind[jj, h]*exp(valcov %*% beta.ind)))
valZ3 = valcov[ , "Z3"]
surv.con = c(exp(-((1-valZ3)*bHaz0con[jj, h] + valZ3*bHaz1con[jj, h])*exp(valcov[ , 1:3] %*% beta.con)))
# SE of predictive residual survival probabilities
dfsurv.ful = dfbeta.ful %*% t(valcov*surv.ful*log(surv.ful)) - dfbHaz.ful %*% t(surv.ful*exp(valcov %*% beta.ful))
dfsurv.srs = dfbeta.srs %*% t(valcov*surv.srs*log(surv.srs)) - dfbHaz.srs %*% t(surv.srs*exp(valcov %*% beta.srs))
dfsurv.ind = dfbeta.ind %*% t(valcov*surv.ind*log(surv.ind)) - dfbHaz.ind %*% t(surv.ind*exp(valcov %*% beta.ind))
survse.ful = sqdg(crossprod(dfsurv.ful)*nlm/(nlm-1))
omega = t(dfsurv.srs[noncase.srs, ]) %*% jcov.srs %*% dfsurv.srs[noncase.srs, ]
survse.srs = sqdg(crossprod(dfsurv.srs/sqrt(fuldat[ind.srs, wgt.srs]))*nlm/(nlm-1) + omega)
omega = t(dfsurv.ind[noncase.ind, ]) %*% jcov.ind %*% dfsurv.ind[noncase.ind, ]
survse.ind = sqdg(crossprod(dfsurv.ind/sqrt(fuldat[ind.ind, wgt.ind]))*nlm/(nlm-1) + omega)
valexplp = c(exp(valcov[ , 1:3] %*% beta.con))
B = valcov[ , 1:3] * (-log(surv.con)) - ((1-valZ3)*valexplp) %o% colSums(bhazZ32sum[failtime<=horizon[h] & Z3==0, -c("Z3", "failtime")])-
(valZ3*valexplp) %o% colSums(bhazZ32sum[failtime<=horizon[h] & Z3==1, -c("Z3", "failtime")])
varH= valexplp^2 * ((1-valZ3)*bhaz.con[failtime<=horizon[h] & case==1 & Z3==0, sum((bhazZ3)^2)] +
valZ3*bhaz.con[failtime<=horizon[h] & case==1 & Z3==1, sum((bhazZ3)^2)]) + diag(B %*% fitcon$var %*% t(B))
survse.con = surv.con*sqrt(varH)
# save predictions
val.ind = valdat[tj==0, T.obs>=landmark.time[jj]]
survful[val.ind, jj, h] = surv.ful
survsrs[val.ind, jj, h] = surv.srs
survind[val.ind, jj, h] = surv.ind
survcon[val.ind, jj, h] = surv.con
survseful[val.ind, jj, h] = survse.ful
survsesrs[val.ind, jj, h] = survse.srs
survseind[val.ind, jj, h] = survse.ind
survsecon[val.ind, jj, h] = survse.con
# validation
surv.true = c(exp(-H_0_solve(horizon[h], jj)*exp(valcov %*% c(beta[jj, ], beta_cor[jj, ], 0.3))))
True = as.numeric(valdat[tj==landmark.time[jj], T.resi] > horizon[h])
ROCful = roc(response=True, predictor=surv.ful, quiet=TRUE)
ROCsrs = roc(response=True, predictor=surv.srs, quiet=TRUE)
ROCind = roc(response=True, predictor=surv.ind, quiet=TRUE)
ROCcon = roc(response=True, predictor=surv.con, quiet=TRUE)
aucful[jj, h] = ROCful$auc
aucsrs[jj, h] = ROCsrs$auc
aucind[jj, h] = ROCind$auc
auccon[jj, h] = ROCcon$auc
bsful[jj, h] = mean((True - surv.ful)^2)
bssrs[jj, h] = mean((True - surv.srs)^2)
bsind[jj, h] = mean((True - surv.ind)^2)
bscon[jj, h] = mean((True - surv.con)^2)
rmseful[jj, h] = sqrt(mean((surv.ful - surv.true)^2))
rmsesrs[jj, h] = sqrt(mean((surv.srs - surv.true)^2))
rmseind[jj, h] = sqrt(mean((surv.ind - surv.true)^2))
rmsecon[jj, h] = sqrt(mean((surv.con - surv.true)^2))
cvrgful[jj, h] = mean((surv.ful - qnorm(0.975)*survse.ful < surv.true) & (surv.true < surv.ful + qnorm(0.975)*survse.ful))
cvrgsrs[jj, h] = mean((surv.srs - qnorm(0.975)*survse.srs < surv.true) & (surv.true < surv.srs + qnorm(0.975)*survse.srs))
cvrgind[jj, h] = mean((surv.ind - qnorm(0.975)*survse.ind < surv.true) & (surv.true < surv.ind + qnorm(0.975)*survse.ind))
cvrgcon[jj, h] = mean((surv.con - qnorm(0.975)*survse.con < surv.true) & (surv.true < surv.con + qnorm(0.975)*survse.con))
}
}
